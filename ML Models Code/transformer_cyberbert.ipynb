{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc986550-18ba-4fb8-81d3-fb4c5b739da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: unitary/toxic-bert\n",
      "Results will be saved to: results/roberta\n",
      "Data directory: 5folds\n",
      "File pattern: ml_data_part0{part_num}.csv\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'roberta'\n",
    "DATA_DIR = '5folds'\n",
    "FILE_PATTERN = 'ml_data_part0{part_num}.csv'\n",
    "RESULTS_DIR = f'results/{MODEL_NAME}'\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "config = {\n",
    "    'model_name': 'unitary/toxic-bert',\n",
    "    'tokenizer_class': 'AutoTokenizer',\n",
    "    'model_class': 'AutoModelForSequenceClassification'\n",
    "}\n",
    "print(f\"Using model: {config['model_name']}\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"File pattern: {FILE_PATTERN}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccc9c0",
   "metadata": {
    "papermill": {
     "duration": 0.004189,
     "end_time": "2026-01-14T06:29:06.113631",
     "exception": false,
     "start_time": "2026-01-14T06:29:06.109442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Task Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b6da962",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "TASK_CONFIG = {\n",
    "    'binary': {\n",
    "        'name': 'Binary Cyberbullying Classification',\n",
    "        'enabled': True,\n",
    "        'num_labels': 2,\n",
    "        'class_names': ['Non-CB', 'CB'],\n",
    "        'filter_cb_only': False,\n",
    "        'problem_type': 'single_label',\n",
    "        'training_function': 'train_transformer_model'\n",
    "    },\n",
    "    'severity': {\n",
    "        'name': 'Severity Classification',\n",
    "        'enabled': True,\n",
    "        'num_labels': 3,\n",
    "        'class_names': ['mild', 'moderate', 'severe'],\n",
    "        'filter_cb_only': True,  # CB only, 3 classes\n",
    "        'problem_type': 'single_label',\n",
    "        'training_function': 'train_transformer_model'\n",
    "    },\n",
    "    'role': {\n",
    "        'name': 'Role Classification',\n",
    "        'enabled': True,\n",
    "        'num_labels': 7,\n",
    "        'class_names': ['bully', 'bully_assistant', 'aggressive_victim', 'aggressive_defender',\n",
    "                       'passive_bystander', 'non_aggressive_victim', 'aggressive_defender_noncb'],\n",
    "        'filter_cb_only': False,\n",
    "        'problem_type': 'single_label',\n",
    "        'training_function': 'train_transformer_model'\n",
    "    },\n",
    "    'topic': {\n",
    "        'name': 'Topic Classification (Multi-label)',\n",
    "        'enabled': True,\n",
    "        'num_labels': 10,\n",
    "        'topic_names': ['disability', 'gender', 'intellectual', 'other', 'physical',\n",
    "                       'political', 'race', 'religious', 'sexual', 'social_status'],\n",
    "        'filter_cb_only': True,\n",
    "        'problem_type': 'multi_label',\n",
    "        'training_function': 'train_multilabel_model'\n",
    "    }\n",
    "}\n",
    "for task_key, task_info in TASK_CONFIG.items():\n",
    "    status = \"ENABLED\" if task_info['enabled'] else \"DISABLED\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f09f9a",
   "metadata": {
    "papermill": {
     "duration": 0.004234,
     "end_time": "2026-01-14T06:29:06.137991",
     "exception": false,
     "start_time": "2026-01-14T06:29:06.133757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8a7508-ebcf-4435-85fb-b6674c57d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    RobertaTokenizer, RobertaForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    balanced_accuracy_score, precision_score,\n",
    "    recall_score, f1_score\n",
    ")\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d444b8",
   "metadata": {
    "papermill": {
     "duration": 0.005373,
     "end_time": "2026-01-14T06:29:10.181158",
     "exception": false,
     "start_time": "2026-01-14T06:29:10.175785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809610d5",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_target(data):\n",
    "    return (data['cyberbullying'] == 't').astype(int)\n",
    "def create_severity_labels(data):\n",
    "    severity_map = {'no_severity': 0, 'mild': 1, 'moderate': 2, 'severe': 3}\n",
    "    severity_labels = data['bullying_severity'].map(severity_map).fillna(0).astype(int)\n",
    "    return severity_labels\n",
    "def create_topic_labels_multilabel(data):\n",
    "    topic_cols = ['has_disability', 'has_gender', 'has_intellectual', 'has_none', 'has_physical',\n",
    "                  'has_political', 'has_race', 'has_religious', 'has_sexual', 'has_social_status']\n",
    "    topic_labels = data[topic_cols].apply(lambda col: (col == 't').astype(int))\n",
    "    return topic_labels.values, topic_cols\n",
    "def create_role_labels(data):\n",
    "    role_map = {\n",
    "        'bully': 0,\n",
    "        'bully_assistant': 1,\n",
    "        'aggressive_victim': 2,\n",
    "        'aggressive_defender': 3,\n",
    "        'passive_bystander': 4,\n",
    "        'non_aggressive_victim': 5,\n",
    "        'aggressive_defender_noncb': 6\n",
    "    }\n",
    "    role_labels = data['bullying_role'].map(role_map).fillna(4).astype(int)  \n",
    "    return role_labels.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83da7e",
   "metadata": {
    "papermill": {
     "duration": 0.004364,
     "end_time": "2026-01-14T06:29:10.208396",
     "exception": false,
     "start_time": "2026-01-14T06:29:10.204032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bde6183",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_metrics(y_true, y_pred, y_proba, class_names, task_name, fold_num, model_name=MODEL_NAME):\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(y_true, y_pred)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics['f1_micro'] = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        metrics['f1_binary'] = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    metrics['f1_per_class'] = f1_score(y_true, y_pred, average=None, zero_division=0).tolist()\n",
    "    metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics['precision_weighted'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics['recall_weighted'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['precision_per_class'] = precision_score(y_true, y_pred, average=None, zero_division=0).tolist()\n",
    "    metrics['recall_per_class'] = recall_score(y_true, y_pred, average=None, zero_division=0).tolist()\n",
    "    try:\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        num_classes = len(np.unique(y_true))\n",
    "        if num_classes == 2:\n",
    "            if y_proba.ndim > 1 and y_proba.shape[1] == 2:\n",
    "                metrics['auroc'] = roc_auc_score(y_true, y_proba[:, 1])\n",
    "            else:\n",
    "                metrics['auroc'] = roc_auc_score(y_true, y_proba)\n",
    "        else:\n",
    "            num_total_classes = len(class_names) if class_names is not None else (y_proba.shape[1] if y_proba is not None and y_proba.ndim > 1 else num_classes)\n",
    "            per_class_auroc = [None] * num_total_classes\n",
    "            aucs = []\n",
    "            supports = []\n",
    "            for c in range(num_total_classes):\n",
    "                y_bin = (y_true == c).astype(int)\n",
    "                if np.unique(y_bin).size < 2:\n",
    "                    per_class_auroc[c] = None\n",
    "                    continue\n",
    "                try:\n",
    "                    auc_c = roc_auc_score(y_bin, y_proba[:, c])\n",
    "                    per_class_auroc[c] = float(auc_c)\n",
    "                    aucs.append(auc_c)\n",
    "                    supports.append(int(y_bin.sum()))\n",
    "                except Exception:\n",
    "                    per_class_auroc[c] = None\n",
    "            metrics['auroc_per_class'] = per_class_auroc\n",
    "            if aucs:\n",
    "                metrics['auroc_macro'] = float(np.mean(aucs))\n",
    "                total = float(np.sum(supports)) if supports else 0.0\n",
    "                metrics['auroc_weighted'] = float(np.sum([a*w for a,w in zip(aucs, supports)]) / total) if total > 0 else float(np.mean(aucs))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    with open(f'{RESULTS_DIR}/{task_name}_metrics_fold_{fold_num}.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2, default=str)\n",
    "    return metrics\n",
    "def save_multilabel_metrics(y_true, y_pred, y_proba, topic_names, task_name, fold_num, model_name=MODEL_NAME):\n",
    "    metrics = {}\n",
    "    topic_metrics = []\n",
    "    balanced_accuracies = []\n",
    "    aurocs = []\n",
    "    balanced_accuracy_per_topic = {}\n",
    "    auroc_per_topic = {}\n",
    "    f1_per_topic = {}\n",
    "    for i, topic_name in enumerate(topic_names):\n",
    "        if y_true[:, i].sum() == 0:\n",
    "            continue\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "            y_true[:, i], y_pred[:, i], average='binary', zero_division=0\n",
    "        )\n",
    "        support = y_true[:, i].sum()\n",
    "        balanced_acc = balanced_accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "        balanced_accuracies.append(balanced_acc)\n",
    "        balanced_accuracy_per_topic[topic_name] = float(balanced_acc)\n",
    "        f1_per_topic[topic_name] = float(f1)\n",
    "        try:\n",
    "            from sklearn.metrics import roc_auc_score\n",
    "            if y_proba.ndim > 1 and y_proba.shape[1] > i:\n",
    "                auroc = roc_auc_score(y_true[:, i], y_proba[:, i])\n",
    "                aurocs.append(auroc)\n",
    "            else:\n",
    "                auroc = np.nan\n",
    "            auroc_per_topic[topic_name] = None if (isinstance(auroc, float) and np.isnan(auroc)) else float(auroc)\n",
    "        except:\n",
    "            auroc = np.nan\n",
    "        auroc_per_topic[topic_name] = None if (isinstance(auroc, float) and np.isnan(auroc)) else float(auroc)\n",
    "        topic_metrics.append({\n",
    "            'topic': topic_name,\n",
    "            'precision': p,\n",
    "            'recall': r,\n",
    "            'f1': f1,\n",
    "            'f1_macro': f1,  \n",
    "            'f1_micro': f1,   \n",
    "            'balanced_accuracy': balanced_acc,\n",
    "            'auroc': auroc,\n",
    "            'support': support\n",
    "        })\n",
    "    subset_accuracy = np.mean(np.all(y_true == y_pred, axis=1))\n",
    "    subset_balanced_accuracy = np.mean(balanced_accuracies)\n",
    "    precisions = [m['precision'] for m in topic_metrics]\n",
    "    recalls = [m['recall'] for m in topic_metrics]\n",
    "    f1s = [m['f1'] for m in topic_metrics]\n",
    "    metrics['subset_accuracy'] = subset_accuracy\n",
    "    metrics['subset_balanced_accuracy'] = subset_balanced_accuracy\n",
    "    metrics['macro_precision'] = np.mean(precisions)\n",
    "    metrics['macro_recall'] = np.mean(recalls)\n",
    "    metrics['macro_f1'] = np.mean(f1s)\n",
    "    metrics['macro_balanced_accuracy'] = np.mean(balanced_accuracies)\n",
    "    metrics['balanced_accuracy_per_topic'] = balanced_accuracy_per_topic\n",
    "    metrics['f1_per_topic'] = f1_per_topic\n",
    "    metrics['auroc_per_topic'] = auroc_per_topic\n",
    "    if aurocs:\n",
    "        metrics['macro_auroc'] = np.mean([a for a in aurocs if not np.isnan(a)])\n",
    "    from sklearn.metrics import f1_score\n",
    "    metrics['micro_f1'] = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    metrics['weighted_f1'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['per_topic_metrics'] = topic_metrics\n",
    "    with open(f'{RESULTS_DIR}/{task_name}_metrics_fold_{fold_num}.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2, default=str)\n",
    "    return metrics\n",
    "def aggregate_metrics(all_fold_metrics):\n",
    "    if not all_fold_metrics:\n",
    "        return {}\n",
    "    agg = {}\n",
    "    first_fold = all_fold_metrics[0]\n",
    "    metric_keys = [\n",
    "        k for k in first_fold.keys()\n",
    "        if k not in ['per_topic_metrics', 'f1_per_class', 'precision_per_class', 'recall_per_class']\n",
    "    ]\n",
    "    for key in metric_keys:\n",
    "        values = [\n",
    "            m[key] for m in all_fold_metrics\n",
    "            if key in m\n",
    "            and isinstance(m[key], (int, float, np.integer, np.floating))\n",
    "            and not np.isnan(m[key])\n",
    "        ]\n",
    "        if values:\n",
    "            agg[f'{key}_mean'] = float(np.mean(values))\n",
    "            agg[f'{key}_std']  = float(np.std(values))\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d29d45b",
   "metadata": {
    "papermill": {
     "duration": 0.004462,
     "end_time": "2026-01-14T06:29:10.256418",
     "exception": false,
     "start_time": "2026-01-14T06:29:10.251956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c0f800a",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx]) if hasattr(self.texts, 'iloc') else str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels.iloc[idx] if hasattr(self.labels, 'iloc') else self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "class MultiLabelTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx]) if hasattr(self.texts, 'iloc') else str(self.texts[idx])\n",
    "        text = text.strip() if text and text != 'nan' else \"\"\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx] if not hasattr(self.labels, 'iloc') else self.labels.iloc[idx], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d0c38",
   "metadata": {
    "papermill": {
     "duration": 0.004511,
     "end_time": "2026-01-14T06:29:10.285860",
     "exception": false,
     "start_time": "2026-01-14T06:29:10.281349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65a878e6-fa0a-4a07-80d9-97dc0359a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_base_model(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' not in name and 'pooler' not in name:\n",
    "            param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "def get_tokenizer_and_model(config, num_labels, problem_type=\"single_label_classification\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config['model_name'],\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        problem_type=problem_type,\n",
    "        classifier_dropout=0.1\n",
    "    ).to(device)\n",
    "    return tokenizer, model\n",
    "\n",
    "def train_transformer_model(train_texts, train_labels, test_texts, test_labels, num_labels, task_name):\n",
    "    tokenizer, model = get_tokenizer_and_model(config, num_labels, \"single_label_classification\")\n",
    "    model = freeze_base_model(model)\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "    test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results_temp',\n",
    "        num_train_epochs=20,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=5e-4,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=None\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    pred_probs = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "    pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    return pred_labels, pred_probs\n",
    "\n",
    "class MultiLabelTransformer(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            problem_type=\"multi_label_classification\",\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "def train_multilabel_model(train_texts, train_labels, test_texts, test_labels, num_labels, task_name):\n",
    "    tokenizer, _ = get_tokenizer_and_model(config, num_labels, \"multi_label_classification\")\n",
    "    model = MultiLabelTransformer(\n",
    "        config['model_name'],\n",
    "        num_labels\n",
    "    ).to(device)\n",
    "    model = freeze_base_model(model)\n",
    "    train_dataset = MultiLabelTextDataset(train_texts, train_labels, tokenizer)\n",
    "    test_dataset = MultiLabelTextDataset(test_texts, test_labels, tokenizer)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results_temp',\n",
    "        num_train_epochs=20,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=5e-4,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=None\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    pred_probs = torch.sigmoid(torch.tensor(predictions.predictions)).numpy()\n",
    "    pred_labels = (pred_probs > 0.5).astype(int)\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    return pred_labels, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef16b8-be55-4f65-b160-7ae99a27d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {task_key: [] for task_key in TASK_CONFIG.keys()}\n",
    "for fold_num in range(1, NUM_FOLDS + 1):\n",
    "    pass\n",
    "    all_partitions = []\n",
    "    for part_num in range(1, 6):\n",
    "        file_path = f\"{DATA_DIR}/{FILE_PATTERN.format(part_num=part_num)}\"\n",
    "        partition = pd.read_csv(file_path)\n",
    "        all_partitions.append(partition)\n",
    "    test_data = all_partitions[fold_num - 1]\n",
    "    train_data = pd.concat([all_partitions[i] for i in range(5) if i != (fold_num - 1)], ignore_index=True)\n",
    "    data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "    train_idx = np.array([True] * len(train_data) + [False] * len(test_data))\n",
    "    test_idx = np.array([False] * len(train_data) + [True] * len(test_data))\n",
    "    text_data = data['comment_content'].fillna('')\n",
    "    target = get_target(data)\n",
    "    severity_labels = create_severity_labels(data)\n",
    "    role_labels = create_role_labels(data)\n",
    "    topic_labels, topic_cols = create_topic_labels_multilabel(data)\n",
    "    for task_key, task_config in TASK_CONFIG.items():\n",
    "        if not task_config['enabled']:\n",
    "            pass\n",
    "            continue\n",
    "        if task_key == 'binary':\n",
    "            train_texts = text_data[train_idx].reset_index(drop=True)\n",
    "            test_texts = text_data[test_idx].reset_index(drop=True)\n",
    "            train_labels = target[train_idx].values\n",
    "            test_labels = target[test_idx].values\n",
    "            num_labels = task_config['num_labels']\n",
    "            class_names = task_config['class_names']\n",
    "        elif task_key == 'severity':\n",
    "            cb_mask = (target == 1).values\n",
    "            cb_train_idx = train_idx & cb_mask\n",
    "            cb_test_idx = test_idx & cb_mask\n",
    "            if cb_train_idx.sum() == 0 or cb_test_idx.sum() == 0:\n",
    "                pass\n",
    "                continue\n",
    "            sev_train = severity_labels[cb_train_idx] - 1\n",
    "            sev_test = severity_labels[cb_test_idx] - 1\n",
    "            valid_train_mask = sev_train >= 0\n",
    "            valid_test_mask = sev_test >= 0\n",
    "            if valid_train_mask.sum() == 0 or valid_test_mask.sum() == 0:\n",
    "                pass\n",
    "                continue\n",
    "            train_texts = text_data[cb_train_idx][valid_train_mask].reset_index(drop=True)\n",
    "            test_texts = text_data[cb_test_idx][valid_test_mask].reset_index(drop=True)\n",
    "            train_labels = sev_train[valid_train_mask]\n",
    "            test_labels = sev_test[valid_test_mask]\n",
    "            num_labels = task_config['num_labels']\n",
    "            class_names = task_config['class_names']\n",
    "        elif task_key == 'role':\n",
    "            train_texts = text_data[train_idx].reset_index(drop=True)\n",
    "            test_texts = text_data[test_idx].reset_index(drop=True)\n",
    "            train_labels = role_labels[train_idx]\n",
    "            test_labels = role_labels[test_idx]\n",
    "            num_labels = task_config['num_labels']\n",
    "            class_names = task_config['class_names']\n",
    "        elif task_key == 'topic':\n",
    "            cb_train_idx = train_idx & (target == 1).values\n",
    "            cb_test_idx = test_idx & (target == 1).values\n",
    "            if cb_train_idx.sum() == 0 or cb_test_idx.sum() == 0:\n",
    "                pass\n",
    "                continue\n",
    "            train_texts = text_data[cb_train_idx].reset_index(drop=True)\n",
    "            test_texts = text_data[cb_test_idx].reset_index(drop=True)\n",
    "            train_labels = topic_labels[cb_train_idx]\n",
    "            test_labels = topic_labels[cb_test_idx]\n",
    "            num_labels = task_config['num_labels']\n",
    "        if task_config['training_function'] == 'train_transformer_model':\n",
    "            pred, pred_probs = train_transformer_model(\n",
    "                train_texts, train_labels,\n",
    "                test_texts, test_labels,\n",
    "                num_labels, f\"{task_key}_classification\"\n",
    "            )\n",
    "            metrics = save_metrics(\n",
    "                test_labels, pred, pred_probs,\n",
    "                class_names,\n",
    "                f'{task_key}_classification',\n",
    "                fold_num\n",
    "            )\n",
    "            if 'auroc' in metrics:\n",
    "                pass\n",
    "            if 'auroc_macro' in metrics:\n",
    "                pass\n",
    "        elif task_config['training_function'] == 'train_multilabel_model':\n",
    "            pred, pred_probs = train_multilabel_model(\n",
    "                train_texts, train_labels,\n",
    "                test_texts, test_labels,\n",
    "                num_labels, f\"{task_key}_classification\"\n",
    "            )\n",
    "            topic_names = task_config['topic_names']\n",
    "            metrics = save_multilabel_metrics(\n",
    "                test_labels, pred, pred_probs,\n",
    "                topic_names,\n",
    "                f'{task_key}_classification',\n",
    "                fold_num\n",
    "            )\n",
    "            if 'balanced_accuracy_per_topic' in metrics:\n",
    "                pass\n",
    "                for _t, _v in metrics['balanced_accuracy_per_topic'].items():\n",
    "                    if _v is None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        pass\n",
    "            if 'auroc_per_topic' in metrics:\n",
    "                pass\n",
    "                for _t, _v in metrics['auroc_per_topic'].items():\n",
    "                    if _v is None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        pass\n",
    "            if 'f1_per_topic' in metrics:\n",
    "                pass\n",
    "                for _t, _v in metrics['f1_per_topic'].items():\n",
    "                    if _v is None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        pass\n",
    "            if 'macro_auroc' in metrics:\n",
    "                pass\n",
    "        all_results[task_key].append(metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf55b8-a541-4081-8adf-ca051b383f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_key, task_config in TASK_CONFIG.items():\n",
    "    if not task_config['enabled'] or not all_results[task_key]:\n",
    "        continue\n",
    "    agg = aggregate_metrics(all_results[task_key])\n",
    "    if task_config['problem_type'] == 'single_label':\n",
    "        pass\n",
    "        if 'auroc_mean' in agg:\n",
    "            pass\n",
    "        if 'auroc_macro_mean' in agg:\n",
    "            pass\n",
    "    elif task_config['problem_type'] == 'multi_label':\n",
    "        pass\n",
    "        if 'macro_auroc_mean' in agg:\n",
    "            pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 57675.881895,
   "end_time": "2026-01-14T22:30:21.226317",
   "environment_variables": {},
   "exception": null,
   "input_path": "transformer_with_Auroc_10T.ipynb",
   "output_path": "transformer_with_Auroc_10T_ROBERTA-output.ipynb",
   "parameters": {},
   "start_time": "2026-01-14T06:29:05.344422",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
