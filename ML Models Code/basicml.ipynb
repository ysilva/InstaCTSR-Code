{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "results_dir = 'results/basicml'\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 2\n",
    "train_data = pd.read_csv(f'data_splits/splits/fold_{fold_num}_train.csv')\n",
    "test_data = pd.read_csv(f'data_splits/splits/fold_{fold_num}_test.csv')\n",
    "\n",
    "train_data = train_data.replace({'f': 0, 't': 1})\n",
    "test_data = test_data.replace({'f': 0, 't': 1})\n",
    "\n",
    "data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "train_idx = pd.Series([True] * len(train_data) + [False] * len(test_data))\n",
    "test_idx = pd.Series([False] * len(train_data) + [True] * len(test_data))\n",
    "\n",
    "print(f\"Using fold {fold_num} - Train: {len(train_data)}, Test: {len(test_data)}, Total: {len(data)}\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(data):\n",
    "    return data['c_cyberbullying_majority'].astype(int)\n",
    "\n",
    "def get_text_features(data):\n",
    "    comments = data['c_comment_content'].fillna('').astype(str)\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    features = vectorizer.fit_transform(comments)\n",
    "    return features, vectorizer\n",
    "\n",
    "def create_severity_labels(data):\n",
    "    severity_cols = ['c_severity_mild_count', 'c_severity_moderate_count', 'c_severity_severe_count']\n",
    "    severity_counts = data[severity_cols].fillna(0)\n",
    "    \n",
    "    cb_mask = data['c_cyberbullying_majority'] == 1\n",
    "    severity_labels = np.zeros(len(data))\n",
    "    \n",
    "    for idx in data.index:\n",
    "        if not cb_mask.iloc[idx]:\n",
    "            severity_labels[idx] = 0\n",
    "        else:\n",
    "            row = severity_counts.iloc[idx]\n",
    "            if row.sum() == 0:\n",
    "                severity_labels[idx] = 0\n",
    "            else:\n",
    "                max_idx = row.argmax()\n",
    "                severity_labels[idx] = max_idx + 1\n",
    "    \n",
    "    return severity_labels.astype(int)\n",
    "\n",
    "def save_basicml_metrics(y_true, y_pred, y_proba, class_names, task_name, model_name):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    metrics['precision_weighted'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['recall_weighted'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    metrics['precision_per_class'] = precision_score(y_true, y_pred, average=None, zero_division=0).tolist()\n",
    "    metrics['recall_per_class'] = recall_score(y_true, y_pred, average=None, zero_division=0).tolist()\n",
    "    metrics['f1_per_class'] = f1_score(y_true, y_pred, average=None, zero_division=0).tolist()\n",
    "    \n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        if y_proba is not None and len(y_proba.shape) > 1 and y_proba.shape[1] >= 2:\n",
    "            metrics['auroc'] = roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            metrics['auroc'] = None\n",
    "    else:\n",
    "        if y_proba is not None and len(y_proba.shape) > 1:\n",
    "            try:\n",
    "                metrics['auroc_macro'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')\n",
    "                metrics['auroc_weighted'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='weighted')\n",
    "            except:\n",
    "                metrics['auroc_macro'] = None\n",
    "                metrics['auroc_weighted'] = None\n",
    "        else:\n",
    "            metrics['auroc_macro'] = None\n",
    "            metrics['auroc_weighted'] = None\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'{model_name.upper()} - {task_name} - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{results_dir}/{model_name}_{task_name}_confusion_matrix_{timestamp}_fold_{fold_num}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    with open(f'{results_dir}/{model_name}_{task_name}_metrics_{timestamp}_fold_{fold_num}.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    np.save(f'{results_dir}/{model_name}_{task_name}_confusion_matrix_{timestamp}_fold_{fold_num}.npy', cm)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def save_multilabel_basicml_metrics(y_true, y_pred, y_proba, topic_names, task_name, model_name):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    topic_metrics = []\n",
    "    for i, topic_name in enumerate(topic_names):\n",
    "        if y_true[:, i].sum() > 0:\n",
    "            p, r, f1, _ = precision_recall_fscore_support(\n",
    "                y_true[:, i], y_pred[:, i], average='binary', zero_division=0\n",
    "            )\n",
    "            support = y_true[:, i].sum()\n",
    "            \n",
    "            try:\n",
    "                if y_proba is not None and len(y_proba.shape) > 1:\n",
    "                    auroc = roc_auc_score(y_true[:, i], y_proba[:, i])\n",
    "                else:\n",
    "                    auroc = None\n",
    "            except:\n",
    "                auroc = None\n",
    "                \n",
    "            topic_metrics.append({\n",
    "                'topic': topic_name,\n",
    "                'precision': p,\n",
    "                'recall': r,\n",
    "                'f1': f1,\n",
    "                'auroc': auroc,\n",
    "                'support': support\n",
    "            })\n",
    "    \n",
    "    subset_accuracy = np.mean(np.all(y_true == y_pred, axis=1))\n",
    "    \n",
    "    if topic_metrics:\n",
    "        precisions = [m['precision'] for m in topic_metrics]\n",
    "        recalls = [m['recall'] for m in topic_metrics]\n",
    "        f1s = [m['f1'] for m in topic_metrics]\n",
    "        aurocs = [m['auroc'] for m in topic_metrics if m['auroc'] is not None]\n",
    "        \n",
    "        metrics['subset_accuracy'] = subset_accuracy\n",
    "        metrics['macro_precision'] = np.mean(precisions)\n",
    "        metrics['macro_recall'] = np.mean(recalls)\n",
    "        metrics['macro_f1'] = np.mean(f1s)\n",
    "        if aurocs:\n",
    "            metrics['macro_auroc'] = np.mean(aurocs)\n",
    "        \n",
    "        metrics['per_topic_metrics'] = topic_metrics\n",
    "        \n",
    "        with open(f'{results_dir}/{model_name}_{task_name}_metrics_{timestamp}_fold_{fold_num}.json', 'w') as f:\n",
    "            json.dump(metrics, f, indent=2, default=str)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = get_target(data)\n",
    "text_features, vec = get_text_features(data)\n",
    "\n",
    "X_train = text_features[train_idx.values]\n",
    "X_test = text_features[test_idx.values]\n",
    "y_train = target[train_idx].values\n",
    "y_test = target[test_idx].values\n",
    "\n",
    "f\"Data: {data.shape}, Train: {train_idx.sum()}, Test: {test_idx.sum()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'LR': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'RF': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'NB': MultinomialNB()\n",
    "}\n",
    "\n",
    "print(\"BINARY CYBERBULLYING CLASSIFICATION:\")\n",
    "print(\"=\" * 50)\n",
    "binary_results = []\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        pred_proba = clf.predict_proba(X_test)\n",
    "    else:\n",
    "        pred_proba = None\n",
    "    \n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_test, pred, average='binary')\n",
    "    \n",
    "    binary_results.append({'Model': name, 'Acc': acc, 'F1': f1})\n",
    "    \n",
    "    metrics = save_basicml_metrics(\n",
    "        y_test, pred, pred_proba, \n",
    "        ['Non-CB', 'CB'], \n",
    "        'binary_classification', \n",
    "        name.lower()\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{name}: Acc={acc:.3f}, F1={f1:.3f}\")\n",
    "    if metrics.get('auroc'):\n",
    "        print(f\"      AUROC={metrics['auroc']:.3f}\")\n",
    "    print(classification_report(y_test, pred, target_names=['Non-CB', 'CB']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_labels = create_severity_labels(data)\n",
    "\n",
    "severity_names = {0: 'none', 1: 'mild', 2: 'moderate', 3: 'severe'}\n",
    "for level, name in severity_names.items():\n",
    "    count = (severity_labels == level).sum()\n",
    "    print(f\"  {level} ({name}): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_mask = (target == 1).values\n",
    "cb_train_idx = train_idx.values & cb_mask\n",
    "cb_test_idx = test_idx.values & cb_mask\n",
    "\n",
    "if cb_train_idx.sum() > 0 and cb_test_idx.sum() > 0:\n",
    "    sev_train_cb = severity_labels[cb_train_idx] - 1\n",
    "    sev_test_cb = severity_labels[cb_test_idx] - 1\n",
    "    \n",
    "    valid_train_mask = sev_train_cb >= 0\n",
    "    valid_test_mask = sev_test_cb >= 0\n",
    "    \n",
    "    X_train_sev = text_features[cb_train_idx][valid_train_mask]\n",
    "    X_test_sev = text_features[cb_test_idx][valid_test_mask]\n",
    "    y_train_sev = sev_train_cb[valid_train_mask]\n",
    "    y_test_sev = sev_test_cb[valid_test_mask]\n",
    "    \n",
    "    severity_names_fixed = ['mild', 'moderate', 'severe']\n",
    "    \n",
    "    for i, name in enumerate(severity_names_fixed):\n",
    "        train_count = (y_train_sev == i).sum()\n",
    "        test_count = (y_test_sev == i).sum()\n",
    "        print(f\"  {i} ({name}): Train={train_count}, Test={test_count}\")\n",
    "    \n",
    "    severity_results = []\n",
    "    for name, clf in classifiers.items():\n",
    "        if name == 'SVM':\n",
    "            clf = SVC(random_state=42, kernel='linear', probability=True)\n",
    "        else:\n",
    "            clf = classifiers[name]\n",
    "        \n",
    "        clf.fit(X_train_sev, y_train_sev)\n",
    "        pred_s = clf.predict(X_test_sev)\n",
    "        \n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            pred_proba_s = clf.predict_proba(X_test_sev)\n",
    "        else:\n",
    "            pred_proba_s = None\n",
    "        \n",
    "        acc_s = accuracy_score(y_test_sev, pred_s)\n",
    "        p_s, r_s, f1_s, _ = precision_recall_fscore_support(y_test_sev, pred_s, average='weighted')\n",
    "        \n",
    "        severity_results.append({'Model': name, 'Acc': acc_s, 'F1': f1_s})\n",
    "        \n",
    "        metrics_s = save_basicml_metrics(\n",
    "            y_test_sev, pred_s, pred_proba_s,\n",
    "            severity_names_fixed,\n",
    "            'severity_classification',\n",
    "            name.lower()\n",
    "        )\n",
    "        \n",
    "        print(f\"{name}: Acc={acc_s:.3f}, F1={f1_s:.3f}\", end=\"\")\n",
    "        if metrics_s.get('auroc_weighted'):\n",
    "            print(f\", AUROC={metrics_s['auroc_weighted']:.3f}\")\n",
    "        else:\n",
    "            print()\n",
    "else:\n",
    "    severity_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_labels_multilabel(data):\n",
    "    topic_cols = ['c_topic_disability_majority', 'c_topic_gender_majority', 'c_topic_intellectual_majority',\n",
    "                  'c_topic_other_majority', 'c_topic_physical_majority', 'c_topic_political_majority',\n",
    "                  'c_topic_race_majority', 'c_topic_religious_majority', 'c_topic_sexual_majority',\n",
    "                  'c_topic_social_status_majority']\n",
    "    \n",
    "    topic_data = data[topic_cols].fillna(0).astype(int)\n",
    "    cb_mask = data['c_cyberbullying_majority'] == 1\n",
    "    \n",
    "    topic_labels = topic_data.copy()\n",
    "    topic_labels[~cb_mask] = 0\n",
    "    \n",
    "    return topic_labels.values, topic_cols\n",
    "\n",
    "topic_labels_ml, topic_cols = create_topic_labels_multilabel(data)\n",
    "topic_names = ['disability', 'gender', 'intellectual', 'other', 'physical', \n",
    "               'political', 'race', 'religious', 'sexual', 'social_status']\n",
    "\n",
    "cb_mask = (target == 1).values\n",
    "cb_topic_labels = topic_labels_ml[cb_mask]\n",
    "for i, name in enumerate(topic_names):\n",
    "    count = cb_topic_labels[:, i].sum()\n",
    "    print(f\"  {name}: {count}\")\n",
    "\n",
    "print(f\"Total CB: {cb_mask.sum()}, With topics: {(cb_topic_labels.sum(axis=1) > 0).sum()}, Multi-topic: {(cb_topic_labels.sum(axis=1) > 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_mask = (target == 1).values\n",
    "cb_train_idx = train_idx.values & cb_mask\n",
    "cb_test_idx = test_idx.values & cb_mask\n",
    "\n",
    "if cb_train_idx.sum() > 0 and cb_test_idx.sum() > 0:\n",
    "    X_train_topic = text_features[cb_train_idx]\n",
    "    X_test_topic = text_features[cb_test_idx]\n",
    "    y_train_topic = topic_labels_ml[cb_train_idx]\n",
    "    y_test_topic = topic_labels_ml[cb_test_idx]\n",
    "    \n",
    "    print(f\"Training: {X_train_topic.shape[0]} CB comments, Testing: {X_test_topic.shape[0]} CB comments\")\n",
    "    \n",
    "    from sklearn.multioutput import MultiOutputClassifier\n",
    "    from sklearn.metrics import hamming_loss, jaccard_score\n",
    "    \n",
    "    topic_results = []\n",
    "    \n",
    "    for name, base_clf in classifiers.items():\n",
    "        if name == 'SVM':\n",
    "            base_clf = SVC(random_state=42, kernel='linear', probability=True)\n",
    "        else:\n",
    "            base_clf = classifiers[name]\n",
    "        \n",
    "        clf = MultiOutputClassifier(base_clf)\n",
    "        clf.fit(X_train_topic, y_train_topic)\n",
    "        pred_t = clf.predict(X_test_topic)\n",
    "        \n",
    "        pred_proba_t = None\n",
    "        if hasattr(base_clf, 'predict_proba'):\n",
    "            try:\n",
    "                pred_proba_t = clf.predict_proba(X_test_topic)\n",
    "                pred_proba_t = np.column_stack([proba[:, 1] for proba in pred_proba_t])\n",
    "            except:\n",
    "                pred_proba_t = None\n",
    "        \n",
    "        subset_accuracy = (pred_t == y_test_topic).all(axis=1).mean()\n",
    "        hamming_loss_score = hamming_loss(y_test_topic, pred_t)\n",
    "        jaccard_avg = jaccard_score(y_test_topic, pred_t, average='samples', zero_division=0)\n",
    "        \n",
    "        topic_metrics = []\n",
    "        for i, topic_name in enumerate(topic_names):\n",
    "            if y_test_topic[:, i].sum() > 0:\n",
    "                p, r, f1, _ = precision_recall_fscore_support(\n",
    "                    y_test_topic[:, i], pred_t[:, i], average='binary', zero_division=0\n",
    "                )\n",
    "                topic_metrics.append({'topic': topic_name, 'precision': p, 'recall': r, 'f1': f1})\n",
    "        \n",
    "        if topic_metrics:\n",
    "            macro_precision = np.mean([m['precision'] for m in topic_metrics])\n",
    "            macro_recall = np.mean([m['recall'] for m in topic_metrics])\n",
    "            macro_f1 = np.mean([m['f1'] for m in topic_metrics])\n",
    "        else:\n",
    "            macro_precision = macro_recall = macro_f1 = 0.0\n",
    "        \n",
    "        topic_results.append({\n",
    "            'Model': name, \n",
    "            'Subset_Acc': subset_accuracy, \n",
    "            'Macro_F1': macro_f1,\n",
    "            'Jaccard': jaccard_avg,\n",
    "            'Hamming_Loss': hamming_loss_score\n",
    "        })\n",
    "        \n",
    "        metrics_t = save_multilabel_basicml_metrics(\n",
    "            y_test_topic, pred_t, pred_proba_t,\n",
    "            topic_names,\n",
    "            'topic_classification',\n",
    "            name.lower()\n",
    "        )\n",
    "        \n",
    "        print(f\"{name}: SubsetAcc={subset_accuracy:.3f}, MacroF1={macro_f1:.3f}, Jaccard={jaccard_avg:.3f}\")\n",
    "else:\n",
    "    topic_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_role_labels(data):\n",
    "    cb_roles = ['c_role_bully_count', 'c_role_cb__bully_assistant_count', \n",
    "                'c_role_cb_aggressive_victim_role_count', 'c_role_cb_aggressive_defender_count']\n",
    "    noncb_roles = ['c_role_noncb_passive_bystander_count', 'c_role_noncb_non_aggressive_victim_count',\n",
    "                   'c_role_noncb_non_aggressive_defender_count']\n",
    "    \n",
    "    role_labels = []\n",
    "    cb_mask = data['c_cyberbullying_majority'] == 1\n",
    "    \n",
    "    for idx in data.index:\n",
    "        if cb_mask.iloc[idx]:\n",
    "            role_counts = data[cb_roles].iloc[idx]\n",
    "            if role_counts.sum() > 0:\n",
    "                max_role = role_counts.argmax()\n",
    "                role_labels.append(max_role)\n",
    "            else:\n",
    "                role_labels.append(0)\n",
    "        else:\n",
    "            role_counts = data[noncb_roles].iloc[idx]\n",
    "            if role_counts.sum() > 0:\n",
    "                max_role = role_counts.argmax()\n",
    "                role_labels.append(max_role + 4)\n",
    "            else:\n",
    "                role_labels.append(4)\n",
    "    \n",
    "    return np.array(role_labels)\n",
    "\n",
    "role_labels = create_role_labels(data)\n",
    "role_names = {0: 'bully', 1: 'bully_assistant', 2: 'aggressive_victim', 3: 'aggressive_defender',\n",
    "              4: 'passive_bystander', 5: 'non_aggressive_victim', 6: 'non_aggressive_defender'}\n",
    "\n",
    "for role_id, name in role_names.items():\n",
    "    count = (role_labels == role_id).sum()\n",
    "    print(f\"  {role_id} ({name}): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_train = role_labels[train_idx.values]\n",
    "role_test = role_labels[test_idx.values]\n",
    "\n",
    "role_results = []\n",
    "for name, clf in classifiers.items():\n",
    "    if name == 'SVM':\n",
    "        clf = SVC(random_state=42, kernel='linear', probability=True)\n",
    "    else:\n",
    "        clf = classifiers[name]\n",
    "    \n",
    "    clf.fit(X_train, role_train)\n",
    "    pred_r = clf.predict(X_test)\n",
    "    \n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        pred_proba_r = clf.predict_proba(X_test)\n",
    "    else:\n",
    "        pred_proba_r = None\n",
    "    \n",
    "    acc_r = accuracy_score(role_test, pred_r)\n",
    "    p_r, r_r, f1_r, _ = precision_recall_fscore_support(role_test, pred_r, average='weighted', zero_division=0)\n",
    "    \n",
    "    role_results.append({'Model': name, 'Acc': acc_r, 'F1': f1_r})\n",
    "    \n",
    "    role_names_list = ['bully', 'bully_assistant', 'aggressive_victim', 'aggressive_defender',\n",
    "                      'passive_bystander', 'non_aggressive_victim', 'non_aggressive_defender']\n",
    "    \n",
    "    metrics_r = save_basicml_metrics(\n",
    "        role_test, pred_r, pred_proba_r,\n",
    "        role_names_list,\n",
    "        'role_classification',\n",
    "        name.lower()\n",
    "    )\n",
    "    \n",
    "    print(f\"{name}: Acc={acc_r:.3f}, F1={f1_r:.3f}\", end=\"\")\n",
    "    if metrics_r.get('auroc_weighted'):\n",
    "        print(f\", AUROC={metrics_r['auroc_weighted']:.3f}\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "feature_names = vec.get_feature_names_out()\n",
    "importance_scores = rf_model.feature_importances_\n",
    "\n",
    "top_indices = np.argsort(importance_scores)[-20:]\n",
    "top_features = [(feature_names[i], importance_scores[i]) for i in top_indices]\n",
    "\n",
    "print(\"Top 20 Features for Cyberbullying Detection:\")\n",
    "for i, (feature, importance) in enumerate(reversed(top_features), 1):\n",
    "    print(f\"{i:2d}. {feature:20s}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(binary_results)\n",
    "best_binary = results_df.loc[results_df['F1'].idxmax()]\n",
    "print(f\"Binary Best: {best_binary['Model']} (F1={best_binary['F1']:.3f}, Acc={best_binary['Acc']:.3f})\")\n",
    "\n",
    "if severity_results:\n",
    "    severity_df = pd.DataFrame(severity_results)\n",
    "    best_severity = severity_df.loc[severity_df['F1'].idxmax()]\n",
    "    print(f\"Severity Best: {best_severity['Model']} (F1={best_severity['F1']:.3f}, Acc={best_severity['Acc']:.3f})\")\n",
    "\n",
    "if topic_results:\n",
    "    topic_df = pd.DataFrame(topic_results)\n",
    "    best_topic = topic_df.loc[topic_df['Macro_F1'].idxmax()]\n",
    "    print(f\"Topic Best: {best_topic['Model']} (MacroF1={best_topic['Macro_F1']:.3f}, SubsetAcc={best_topic['Subset_Acc']:.3f})\")\n",
    "\n",
    "role_df = pd.DataFrame(role_results)\n",
    "best_role = role_df.loc[role_df['F1'].idxmax()]\n",
    "print(f\"Role Best: {best_role['Model']} (F1={best_role['F1']:.3f}, Acc={best_role['Acc']:.3f})\")\n",
    "\n",
    "print(f\"Dataset: {len(data)} samples, Train/Test: {train_idx.sum()}/{test_idx.sum()}\")\n",
    "print(f\"CB prevalence: {(target==1).mean():.1%}, Sessions: {data['s_unit_id'].nunique()}\")\n",
    "\n",
    "cb_mask = (target == 1).values\n",
    "cb_train = (train_idx.values & cb_mask).sum()\n",
    "cb_test = (test_idx.values & cb_mask).sum()\n",
    "print(f\"CB Train/Test: {cb_train}/{cb_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
