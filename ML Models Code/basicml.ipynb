{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "config",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '5folds'\n",
    "FILE_PATTERN = 'ml_data_part0{part_num}.csv'\n",
    "RESULTS_DIR = 'results/basicml_5fold'\n",
    "NUM_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {
    "papermill": {
     "duration": 0.001797,
     "end_time": "2026-01-05T15:43:15.808164",
     "exception": false,
     "start_time": "2026-01-05T15:43:15.806367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions-header",
   "metadata": {
    "papermill": {
     "duration": 0.002318,
     "end_time": "2026-01-05T15:43:16.367336",
     "exception": false,
     "start_time": "2026-01-05T15:43:16.365018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54bc811a-8e69-4fc1-a420-c57cde8656b3",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_target(data):\n",
    "    return (data['cyberbullying'] == 't').astype(int)\n",
    "    \n",
    "def get_text_features(data):\n",
    "    comments = data['comment_content'].fillna('').astype(str)\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    features = vectorizer.fit_transform(comments)\n",
    "    return features, vectorizer\n",
    "\n",
    "def get_severity_labels(data):\n",
    "    severity_map = {'mild': 1, 'moderate': 2, 'severe': 3}\n",
    "    severity_labels = data['bullying_severity'].map(severity_map).fillna(0).astype(int)\n",
    "    return severity_labels\n",
    "    \n",
    "def get_topic_labels_multilabel(data):\n",
    "    topic_cols = [\n",
    "        'has_race', 'has_political', 'has_intellectual', 'has_physical',\n",
    "        'has_social_status', 'has_gender', 'has_none', 'has_religious',\n",
    "        'has_disability', 'has_sexual'\n",
    "    ]\n",
    "    topic_data = data[topic_cols].replace({'t': 1, 'f': 0}).fillna(0).astype(int)\n",
    "    cb_mask = (data['cyberbullying'] == 't').values\n",
    "    topic_labels = topic_data.copy()\n",
    "    topic_labels.loc[~cb_mask] = 0\n",
    "    return topic_labels.values, topic_cols\n",
    "    \n",
    "def get_role_labels(data):\n",
    "    role_map = {\n",
    "        'bully': 0,\n",
    "        'bully_assistant': 1,\n",
    "        'non_aggressive_defender': 2,\n",
    "        'non_aggressive_victim': 3,\n",
    "        'passive_bystander': 4,\n",
    "        'aggressive_defender': 5,\n",
    "        'aggressive_victim': 6\n",
    "    }\n",
    "    role_labels = data['bullying_role'].map(role_map).fillna(0).astype(int)\n",
    "    return role_labels.values\n",
    "    \n",
    "def save_basicml_metrics(y_true, y_pred, class_names, task_name, model_name, fold_num, auroc=None):\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1_micro': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'f1_per_class': f1_score(y_true, y_pred, average=None, zero_division=0).tolist(),\n",
    "        'precision_weighted': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall_weighted': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'precision_per_class': precision_score(y_true, y_pred, average=None, zero_division=0).tolist(),\n",
    "        'recall_per_class': recall_score(y_true, y_pred, average=None, zero_division=0).tolist(),\n",
    "    }\n",
    "    if auroc is not None:\n",
    "        metrics['auroc'] = auroc\n",
    "    with open(f'{RESULTS_DIR}/{model_name}_{task_name}_metrics_fold_{fold_num}.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    return metrics\n",
    "    \n",
    "def save_multilabel_basicml_metrics(y_true, y_pred, topic_names, task_name, model_name, fold_num,\n",
    "                                   f1_macro_labels=None, f1_micro=None,\n",
    "                                   f1_weighted=None, auroc_macro=None, topic_metrics_list=None, overall_balanced_accuracy=None):\n",
    "    metrics = {\n",
    "        'overall_balanced_accuracy': overall_balanced_accuracy,\n",
    "        'f1_macro_labels': f1_macro_labels,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "    }\n",
    "    if auroc_macro is not None:\n",
    "        metrics['auroc_macro'] = auroc_macro\n",
    "    if topic_metrics_list:\n",
    "        metrics['per_topic_metrics'] = topic_metrics_list\n",
    "    with open(f'{RESULTS_DIR}/{model_name}_{task_name}_metrics_fold_{fold_num}.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2, default=str)\n",
    "    return metrics\n",
    "    \n",
    "def aggregate_fold_results(fold_results):\n",
    "    if not fold_results:\n",
    "        return {}\n",
    "    aggregated = {}\n",
    "    for metric_name in fold_results[0].keys():\n",
    "        if metric_name not in ['per_topic_metrics', 'precision_per_class', 'recall_per_class', 'f1_per_class']:\n",
    "            values = [fold.get(metric_name) for fold in fold_results if fold.get(metric_name) is not None]\n",
    "            if values:\n",
    "                aggregated[f'{metric_name}_mean'] = np.mean(values)\n",
    "                aggregated[f'{metric_name}_std'] = np.std(values)\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {
    "papermill": {
     "duration": 0.003184,
     "end_time": "2026-01-05T15:43:16.389555",
     "exception": false,
     "start_time": "2026-01-05T15:43:16.386371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Task and Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "setup",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    'LR': {\n",
    "        'classifier': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'name': 'LR',\n",
    "        'enabled': True\n",
    "    },\n",
    "    'RF': {\n",
    "        'classifier': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'name': 'RF',\n",
    "        'enabled': True\n",
    "    },\n",
    "    'SVM': {\n",
    "        'classifier': SVC(random_state=42, probability=True),\n",
    "        'name': 'SVM',\n",
    "        'enabled': True\n",
    "    },\n",
    "    'NB': {\n",
    "        'classifier': MultinomialNB(),\n",
    "        'name': 'NB',\n",
    "        'enabled': True\n",
    "    }\n",
    "}\n",
    "\n",
    "TASK_CONFIG = {\n",
    "    'binary': {\n",
    "        'name': 'Binary Cyberbullying Classification',\n",
    "        'task_key': 'binary',\n",
    "        'enabled': True,\n",
    "        'filter_cb_only': False,  \n",
    "        'class_names': ['Non-CB', 'CB'],\n",
    "        'metric_type': 'single_label'\n",
    "    },\n",
    "    'severity': {\n",
    "        'name': 'Severity Classification',\n",
    "        'task_key': 'severity',\n",
    "        'enabled': True,\n",
    "        'filter_cb_only': True, \n",
    "        'class_names': ['mild', 'moderate', 'severe'],\n",
    "        'metric_type': 'single_label'\n",
    "    },\n",
    "    'topic': {\n",
    "        'name': 'Topic Classification (Multi-label)',\n",
    "        'task_key': 'topic',\n",
    "        'enabled': True,\n",
    "        'filter_cb_only': True,  \n",
    "        'topic_names': ['has_race', 'has_political', 'has_intellectual', 'has_physical',\n",
    "                       'has_social_status', 'has_gender', 'has_none', 'has_religious',\n",
    "                       'has_disability', 'has_sexual'],\n",
    "        'metric_type': 'multi_label',\n",
    "        'use_multioutput': True\n",
    "    },\n",
    "    'role': {\n",
    "        'name': 'Role Classification',\n",
    "        'task_key': 'role',\n",
    "        'enabled': True,\n",
    "        'filter_cb_only': False,  \n",
    "        'class_names': ['bully', 'bully_assistant', 'non_aggressive_defender', 'non_aggressive_victim',\n",
    "                       'passive_bystander', 'aggressive_defender', 'aggressive_victim'],\n",
    "        'metric_type': 'single_label'\n",
    "    }\n",
    "}\n",
    "all_results = {task_key: defaultdict(list) for task_key in TASK_CONFIG.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dbffbc-a7e1-4e14-8672-57e343eac8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_num in range(1, NUM_FOLDS + 1):\n",
    "    all_partitions = []\n",
    "    for part_num in range(1, 6):\n",
    "        filepath = os.path.join(DATA_DIR, FILE_PATTERN.format(part_num=part_num))\n",
    "        partition = pd.read_csv(filepath)\n",
    "        all_partitions.append(partition)\n",
    "    test_data = all_partitions[fold_num - 1]\n",
    "    train_data = pd.concat([all_partitions[i] for i in range(5) if i != (fold_num - 1)], ignore_index=True)\n",
    "    data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "    train_idx = pd.Series([True] * len(train_data) + [False] * len(test_data))\n",
    "    test_idx = pd.Series([False] * len(train_data) + [True] * len(test_data))\n",
    "    target = get_target(data)\n",
    "    text_features, vec = get_text_features(data)\n",
    "    X_train_base = text_features[train_idx.values]\n",
    "    X_test_base = text_features[test_idx.values]\n",
    "    y_train_base = target[train_idx].values\n",
    "    y_test_base = target[test_idx].values\n",
    "    for task_key, task_config in TASK_CONFIG.items():\n",
    "        if not task_config['enabled']:\n",
    "            continue\n",
    "        if task_key == 'binary':\n",
    "            X_train = X_train_base\n",
    "            X_test = X_test_base\n",
    "            y_train = y_train_base\n",
    "            y_test = y_test_base\n",
    "        elif task_key == 'severity':\n",
    "            severity_labels = get_severity_labels(data)\n",
    "            cb_mask = (target == 1).values\n",
    "            if cb_mask.sum() == 0:\n",
    "                pass\n",
    "                continue\n",
    "            severity_train = severity_labels[train_idx.values][y_train_base == 1]\n",
    "            severity_test = severity_labels[test_idx.values][y_test_base == 1]\n",
    "            X_train_sev = X_train_base[y_train_base == 1]\n",
    "            X_test_sev = X_test_base[y_test_base == 1]\n",
    "            valid_train = severity_train > 0\n",
    "            valid_test = severity_test > 0\n",
    "            if valid_train.sum() == 0 or valid_test.sum() == 0:\n",
    "                pass\n",
    "                continue\n",
    "            X_train = X_train_sev[valid_train]\n",
    "            X_test = X_test_sev[valid_test]\n",
    "            y_train = severity_train[valid_train]\n",
    "            y_test = severity_test[valid_test]\n",
    "        elif task_key == 'topic':\n",
    "            topic_labels_ml, topic_cols = get_topic_labels_multilabel(data)\n",
    "            cb_mask = (target == 1).values\n",
    "            cb_train_idx = train_idx.values & cb_mask\n",
    "            cb_test_idx = test_idx.values & cb_mask\n",
    "            if cb_train_idx.sum() == 0 or cb_test_idx.sum() == 0:\n",
    "                pass\n",
    "                continue\n",
    "            X_train = text_features[cb_train_idx]\n",
    "            X_test = text_features[cb_test_idx]\n",
    "            y_train = topic_labels_ml[cb_train_idx]\n",
    "            y_test = topic_labels_ml[cb_test_idx]\n",
    "        elif task_key == 'role':\n",
    "            role_labels = get_role_labels(data)\n",
    "            X_train = X_train_base\n",
    "            X_test = X_test_base\n",
    "            y_train = role_labels[train_idx.values]\n",
    "            y_test = role_labels[test_idx.values]\n",
    "        for model_key, model_config in MODEL_CONFIG.items():\n",
    "            if not model_config['enabled']:\n",
    "                continue\n",
    "            model_name = model_config['name']\n",
    "            if model_key == 'SVM':\n",
    "                clf = SVC(random_state=42, kernel='linear', probability=True)\n",
    "            else:\n",
    "                clf = model_config['classifier']\n",
    "            if task_config['metric_type'] == 'multi_label' and task_config.get('use_multioutput', False):\n",
    "                clf = MultiOutputClassifier(clf)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            if task_config['metric_type'] == 'single_label':\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "                f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                f1_micro = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "                f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "                f1_per_class = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
    "                try:\n",
    "                    if hasattr(clf, \"predict_proba\"):\n",
    "                        y_proba = clf.predict_proba(X_test)\n",
    "                        n_classes = len(np.unique(y_test))\n",
    "                        if n_classes == 2:\n",
    "                            auroc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "                        else:\n",
    "                            auroc_ovr = roc_auc_score(y_test, y_proba, average='macro', multi_class='ovr')\n",
    "                            auroc_ovo = roc_auc_score(y_test, y_proba, average='macro', multi_class='ovo')\n",
    "                            auroc = auroc_ovr\n",
    "                    else:\n",
    "                        auroc = None\n",
    "                except Exception as e:\n",
    "                    auroc = None\n",
    "                metrics = save_basicml_metrics(\n",
    "                    y_test, y_pred, task_config['class_names'],\n",
    "                    f\"{task_key}_classification\", model_name.lower(), fold_num,\n",
    "                    auroc=auroc\n",
    "                )\n",
    "                if auroc is not None:\n",
    "                    pass\n",
    "                else:\n",
    "                    pass\n",
    "                all_results[task_key][model_name].append(metrics)\n",
    "            elif task_config['metric_type'] == 'multi_label':\n",
    "                balanced_accuracies = []\n",
    "                f1_scores_per_topic = []\n",
    "                auroc_per_topic = []\n",
    "                topic_metrics_list = []\n",
    "                for i, topic_name in enumerate(task_config['topic_names']):\n",
    "                    ba = balanced_accuracy_score(y_test[:, i], y_pred[:, i])\n",
    "                    balanced_accuracies.append(ba)\n",
    "                    f1 = f1_score(y_test[:, i], y_pred[:, i], average='binary', zero_division=0)\n",
    "                    f1_scores_per_topic.append(f1)\n",
    "                    p = precision_score(y_test[:, i], y_pred[:, i], average='binary', zero_division=0)\n",
    "                    r = recall_score(y_test[:, i], y_pred[:, i], average='binary', zero_division=0)\n",
    "                    auroc_topic = None\n",
    "                    try:\n",
    "                        if hasattr(clf.estimators_[i] if hasattr(clf, 'estimators_') else clf, \"predict_proba\"):\n",
    "                            if hasattr(clf, 'estimators_'):\n",
    "                                y_proba_topic = clf.estimators_[i].predict_proba(X_test)[:, 1]\n",
    "                            else:\n",
    "                                y_proba_topic = clf.predict_proba(X_test)[:, i]\n",
    "                            auroc_topic = roc_auc_score(y_test[:, i], y_proba_topic)\n",
    "                            auroc_per_topic.append(auroc_topic)\n",
    "                    except:\n",
    "                        pass\n",
    "                    topic_metrics_list.append({\n",
    "                        'topic': topic_name,\n",
    "                        'f1': f1,\n",
    "                        'precision': p,\n",
    "                        'recall': r,\n",
    "                        'auroc': auroc_topic,\n",
    "                        'balanced_accuracy': ba\n",
    "                    })\n",
    "                overall_balanced_accuracy = np.mean(balanced_accuracies)\n",
    "                f1_macro_labels = np.mean(f1_scores_per_topic)\n",
    "                f1_micro = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "                f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "                auroc_macro = np.mean(auroc_per_topic) if len(auroc_per_topic) > 0 else None\n",
    "                metrics = save_multilabel_basicml_metrics(\n",
    "                    y_test, y_pred, task_config['topic_names'],\n",
    "                    f\"{task_key}_classification\", model_name.lower(), fold_num,\n",
    "                                        f1_macro_labels=f1_macro_labels,\n",
    "                    f1_micro=f1_micro,\n",
    "                    f1_weighted=f1_weighted,\n",
    "                    auroc_macro=auroc_macro,\n",
    "                    topic_metrics_list=topic_metrics_list,\n",
    "                    overall_balanced_accuracy=overall_balanced_accuracy\n",
    "                )\n",
    "                if auroc_macro is not None:\n",
    "                    pass\n",
    "                else:\n",
    "                    pass\n",
    "                all_results[task_key][model_name].append(metrics)\n",
    "print(\"ALL FOLDS COMPLETED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_results = {}\n",
    "for task_key, task_config in TASK_CONFIG.items():\n",
    "    if not task_config['enabled']:\n",
    "        continue\n",
    "    final_results[f\"{task_key}_classification\"] = {}\n",
    "    for model_key, model_config in MODEL_CONFIG.items():\n",
    "        if not model_config['enabled']:\n",
    "            continue\n",
    "        model_name = model_config['name']\n",
    "        if all_results[task_key][model_name]:\n",
    "            final_results[f\"{task_key}_classification\"][model_name] = \\\n",
    "                aggregate_fold_results(all_results[task_key][model_name])\n",
    "final_results['num_folds'] = NUM_FOLDS\n",
    "final_results['timestamp'] = timestamp\n",
    "output_path = f'{RESULTS_DIR}/aggregated_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1377.392862,
   "end_time": "2026-01-05T16:06:12.427625",
   "environment_variables": {},
   "exception": true,
   "input_path": "basicml_looped_structure_metrics_fixed.ipynb",
   "output_path": "basicml_looped_structure_metrics_fixed.ipynb",
   "parameters": {},
   "start_time": "2026-01-05T15:43:15.034763",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
